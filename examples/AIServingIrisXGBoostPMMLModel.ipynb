{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing Iris XGBoost PMML Model using AI-Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PMML stands for Predictive Model Markup Language. It is the de facto standard to present the classic machine learning models. With PMML, it is easy to develop a model on one system using one application and deploy the model on another system using another application.\n",
    "\n",
    "In this tutorial, we will use the PMML to show how to deploy the famous Iris classifier using AI-Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"contents\"></a>Contents\n",
    "This notebook contains the following parts:\n",
    "\n",
    "**[Setup](#setup)**<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Prerequisites to run the notebook](#prerequisites)<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Additional information about `ai_serving_pb2.py`](#additional)<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Install dependencies](#dependencies)<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Import dependent modules](#import)<br />\n",
    "**[Validate server](#validate)**<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Define the base HTTP URL](#httpurl)<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Test the server availability](#testserver)<br />\n",
    "**[Deploy the PMML model](#deploy)**<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Deploy the Iris XGBoost model into AI-Serving](#deploy-pmml)<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Retrieve metadata of the deployed model](#metadata)<br />\n",
    "**[Make predictions](#predictions)**<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Prepare the testing records](#test-data)<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[HTTP request formats](#request)<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[JSON requests](#json-request)<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Construct JSON requests](#construct-json-request)<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Make the HTTP requests with JSON data](#make-json-request)<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Consume the HTTP response with JSON data](#consume-json-response)<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[Binary requests](#binary-request)<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Construct binary requests](#construct-binary-request)<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Make the HTTP requests with binary data](#make-binary-request)<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Consume the HTTP responses with binary data](#consume-binary-response)<br />\n",
    "**[Next steps](#next-steps)**<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"setup\"></a>Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"prerequisites\"></a>Prerequisites to run the notebook\n",
    "\n",
    "Run a docker container of AI-Serving. The port `9090` is the port of HTTP endpoint while `9091` is for gRPC, you could see an error likes `Bind for 0.0.0.0:9090 failed: port is already allocated`, then use another new port instead of the first part as follows `-p $(NEW_PORT):9090` to run the command again, and remember the port is always needed in the URL of HTTP endpoint. It will aslo pull the latest docker image of AI-Serving from docker hub if it hvaen't been downloaded yet. Please, refer to [Docker Containers for AI-Serving](https://github.com/autodeployai/ai-serving/tree/master/dockerfiles) about more docker images.\n",
    "\n",
    "```bash\n",
    "docker run --rm -it -v $(PWD):/opt/ai-serving -p 9090:9090 -p 9091:9091 autodeployai/ai-serving\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"additional\"></a>Additional information about `ai_serving_pb2.py`\n",
    "In the current directory, there is a python file `ai_serving_pb2.py`, which is generated from compiling the [ai-serving.proto](https://github.com/autodeployai/ai-serving/tree/master/src/main/protobuf/ai-serving.proto) using [protoc](https://developers.google.com/protocol-buffers/docs/pythontutorial), for example, the command as follows:\n",
    "\n",
    "```bash\n",
    "protoc -I=$SRC_DIR --python_out=. ai-serving.proto\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"dependencies\"></a>Install dependencies\n",
    "We will install python libraries for HTTP request and data manipulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"import\"></a>Import dependent modules\n",
    "Import some dependent modules that we are going to need to run the Iris XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "import ai_serving_pb2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"validate\"></a>Validate server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"httpurl\"></a>Define the base HTTP URL\n",
    "Change the port number `9090` to the appropriate port number if you had changed it during AI-Serving docker instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 9090\n",
    "base_url = 'http://localhost:' + str(port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"testserver\"><a>Test the server availability\n",
    "Use the specific endpoint `http://host:port/up` to test whether the server has been initialized and is ready to accept requests. The `OK` message indicates it's already available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The status of the server:  OK\n"
     ]
    }
   ],
   "source": [
    "test_url = base_url + '/up'\n",
    "response = requests.get(test_url)\n",
    "print('The status of the server: ', response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"deploy\"></a>Deploy the PMML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"deploy-pmml\">Deploy the Iris XGBoost model into AI-Serving\n",
    "First, we need to deploy the PMML model `xgb-iris.pmml` into AI-Serving, which can serve multiple models or multiple versions for a named model at once. The PMML model was generated by the notebook [`Training an Iris classifier using XGBoost`](https://github.com/autodeployai/ai-serving/blob/master/examples/IrisXGBoost.ipynb).\n",
    "\n",
    "You must specify a correct content type for PMML models when constructing an HTTP request to deploy a PMML model, the candidates are:\n",
    " * application/xml\n",
    " * text/xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The depoyment response:  {'name': 'iris', 'version': 1}\n"
     ]
    }
   ],
   "source": [
    "# The specified servable name\n",
    "model_name = 'iris'\n",
    "deployment_url = base_url + '/v1/models/' + model_name\n",
    "\n",
    "# The specified content type for the model:\n",
    "headers = {'Content-Type': 'application/xml'}\n",
    "\n",
    "model_path = os.path.join('models', 'xgb-iris.pmml')\n",
    "with open(model_path, 'rb') as file:\n",
    "    deployment_response = requests.put(deployment_url, headers=headers, data=file)\n",
    "\n",
    "# The response is a JSON object contains the sepcified servable name and the model version deployed\n",
    "deployment_response_info = deployment_response.json()\n",
    "print('The depoyment response: ', deployment_response_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"metadata\"></a>Retrieve metadata of the deployed model\n",
    "The metadata will contain model inputs and outputs, which are needed when constructing an input request and consume an output response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model metadata response:\n",
      "{'createdAt': '2024-10-09T20:54:01',\n",
      " 'id': '02ae4650-0e17-472d-a919-95edb1bbde21',\n",
      " 'latestVersion': 1,\n",
      " 'name': 'iris',\n",
      " 'updateAt': '2024-10-09T20:54:01',\n",
      " 'versions': [{'algorithm': 'MiningModel',\n",
      "               'app': 'Nyoka',\n",
      "               'appVersion': '5.5.0',\n",
      "               'copyright': 'Copyright (c) 2021 Software AG',\n",
      "               'createdAt': '2024-10-09T20:54:01',\n",
      "               'description': 'Default description',\n",
      "               'formatVersion': '4.4.1',\n",
      "               'functionName': 'classification',\n",
      "               'hash': 'cedae9d98bced8fe67de18318d31b209',\n",
      "               'inputs': [{'name': 'sepal length (cm)',\n",
      "                           'optype': 'continuous',\n",
      "                           'type': 'double'},\n",
      "                          {'name': 'sepal width (cm)',\n",
      "                           'optype': 'continuous',\n",
      "                           'type': 'double'},\n",
      "                          {'name': 'petal length (cm)',\n",
      "                           'optype': 'continuous',\n",
      "                           'type': 'double'},\n",
      "                          {'name': 'petal width (cm)',\n",
      "                           'optype': 'continuous',\n",
      "                           'type': 'double'}],\n",
      "               'outputs': [{'name': 'species_probability_0',\n",
      "                            'optype': 'continuous',\n",
      "                            'type': 'double'},\n",
      "                           {'name': 'species_probability_1',\n",
      "                            'optype': 'continuous',\n",
      "                            'type': 'double'},\n",
      "                           {'name': 'species_probability_2',\n",
      "                            'optype': 'continuous',\n",
      "                            'type': 'double'},\n",
      "                           {'name': 'predicted_species',\n",
      "                            'optype': 'nominal',\n",
      "                            'type': 'integer'}],\n",
      "               'runtime': 'PMML4S',\n",
      "               'serialization': 'pmml',\n",
      "               'size': 45693,\n",
      "               'targets': [{'name': 'species',\n",
      "                            'optype': 'nominal',\n",
      "                            'type': 'integer',\n",
      "                            'values': '0,1,2'}],\n",
      "               'type': 'PMML',\n",
      "               'version': 1}]}\n"
     ]
    }
   ],
   "source": [
    "model_version = deployment_response_info['version']\n",
    "metadata_url = base_url + '/v1/models/' + model_name + '/versions/' + str(model_version)\n",
    "metadata_response = requests.get(metadata_url)\n",
    "metadata_response_json = metadata_response.json()\n",
    "\n",
    "# Model info of the specified version\n",
    "model_info = metadata_response_json['versions'][0]\n",
    "\n",
    "# Extra some key values: the inputs and outputs\n",
    "inputs = [x['name'] for x in model_info['inputs']]\n",
    "outpus = [x['name'] for x in model_info['outputs']]\n",
    "\n",
    "# Show the metadata result in json\n",
    "print('The model metadata response:')\n",
    "pprint(metadata_response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"predictions\"></a>Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"test-data\"></a>Prepare the testing records\n",
    "We will use the following testing records in different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_records = [[5.7, 4.4, 1.5, 0.4], [6.4, 2.8, 5.6, 2.1]]\n",
    "map_records = [{'sepal length (cm)': 5.7, \n",
    "                'sepal width (cm)': 4.4,\n",
    "                'petal length (cm)': 1.5,\n",
    "                'petal width (cm)': 0.4}, \n",
    "               {'sepal length (cm)': 6.4, \n",
    "                'sepal width (cm)': 2.8,\n",
    "                'petal length (cm)': 5.6,\n",
    "                'petal width (cm)': 2.1},]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"request\">HTTP request formats for the AI-Serving\n",
    "The request for AI-Serving could have two formats: JSON and binary, the HTTP header Content-Type tells the server which format to handle and thus it is required for all requests. The binary payload has better latency, especially for the big tensor value for ONNX models, while the JSON format is easy for human readability.\n",
    "\n",
    "- Content-Type: application/json. The request body must be a JSON object formatted as described [here](https://github.com/autodeployai/ai-serving#4-predict-api).\n",
    "\n",
    "\n",
    "- Content-Type: application/octet-stream, application/vnd.google.protobuf or application/x-protobuf. The request body must be the protobuf message PredictRequest, besides of those common scalar values, it can use the standard onnx.TensorProto value directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"json-request\"></a>JSON requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"construct-json-request\"></a>Construct JSON requests\n",
    "We will create both JSON objects, one is using the `Records` format that contains a single record, the other is using `Split` format that contains two records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a JSON object using `records` that contains a single record.\n",
    "request_json_recoreds = {\n",
    "    'X': map_records\n",
    "}\n",
    "\n",
    "# Create a JSON object using `split` that contains two records with a filter that\n",
    "# only the output `predicted_Species` is expected.\n",
    "request_json_split = {\n",
    "    'X': {\n",
    "        'columns': inputs,\n",
    "        'data': list_records\n",
    "    },\n",
    "    'filter': ['predicted_species']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"make-json-request\">Make the HTTP requests with JSON data\n",
    "Make predictions using the AI-Serving, the content type of requests with JSON data must be `application/json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When version is omitted, the latest version is used.\n",
    "prediction_url = base_url + '/v1/models/' + model_name\n",
    "\n",
    "# The Content-Type: application/json is specified implicitly when using json instead of data\n",
    "prediction_json_response_records = requests.post(prediction_url, json=request_json_recoreds)\n",
    "prediction_json_response_split = requests.post(prediction_url, json=request_json_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"consume-json-response\">Consume the HTTP response with JSON data\n",
    "Having received the results from the server, we are going to parse the JSON text that we just received for us to make sense of the results. **NOTE: The data format of the output response is always the same as the input request.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON prediction response of the request using `records`:\n",
      "{'result': [{'predicted_species': 0,\n",
      "             'species_probability_0': 0.5506691017471723,\n",
      "             'species_probability_1': 0.22587241877164385,\n",
      "             'species_probability_2': 0.22345847948118383},\n",
      "            {'predicted_species': 2,\n",
      "             'species_probability_0': 0.22347717521202046,\n",
      "             'species_probability_1': 0.22720021905738896,\n",
      "             'species_probability_2': 0.5493226057305906}]}\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "JSON prediction response of the request using `records` with a filter:\n",
      "{'result': {'columns': ['predicted_species'], 'data': [[0], [2]]}}\n"
     ]
    }
   ],
   "source": [
    "print('JSON prediction response of the request using `records`:')\n",
    "pprint(prediction_json_response_records.json())\n",
    "\n",
    "print('-'*120)\n",
    "\n",
    "# Only the predicton column `predicted_Species` is expected.\n",
    "print('JSON prediction response of the request using `records` with a filter:')\n",
    "pprint(prediction_json_response_split.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"binary-request\"></a>Binary requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"construct-binary-request\"></a>Construct binary requests\n",
    "We will create both instances of PredictRequest, one is using the `Records` format that contains a single record, the other is using the `Split` format that contains two records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_serving_pb2 import RecordSpec, Record, PredictRequest, ListValue, Value\n",
    "\n",
    "# Create an instance of RecordSpec using `records` that contains a single record.\n",
    "request_message_records = PredictRequest(X=RecordSpec(\n",
    "    records=[ Record(fields={y[0]: Value(number_value=y[1]) for y in x.items()}) for x in map_records ]))\n",
    "\n",
    "# Create an instance of RecordSpec using `split` that contains two records with a filter that\n",
    "# only the output `predicted_Species` is expected.\n",
    "request_message_split = PredictRequest(\n",
    "    X=RecordSpec(\n",
    "        columns = inputs,\n",
    "        data = [ ListValue(values=[Value(number_value=y) for y in x]) for x in list_records ]),\n",
    "    filter=['predicted_species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"make-binary-request\"></a>Make the HTTP requests with binary data\n",
    "Make predictions using the AI-Serving, the content type of requests with binary data must be one of those three candidates above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Content-Type': 'application/x-protobuf'}\n",
    "\n",
    "# When version is omitted, the latest version is used.\n",
    "prediction_url = base_url + '/v1/models/' + model_name\n",
    "\n",
    "# Make prediction for the `records` request message.\n",
    "prediction_response_records = requests.post(prediction_url, \n",
    "                                           headers=headers, \n",
    "                                           data=request_message_records.SerializeToString())\n",
    "\n",
    "# Make prediciton for the `split` request message.\n",
    "prediction_response_split = requests.post(prediction_url, \n",
    "                                           headers=headers, \n",
    "                                           data=request_message_split.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"consume-binary-response\"></a>Consume the HTTP response with binary data\n",
    "Having received the results from the server, we are going to parse the \"serialized\" message that we just received for us to make sense of the results. **NOTE: The data format of the output response is always the same as the input request.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary prediction response of the request using `records`:\n",
      "result {\n",
      "  records {\n",
      "    fields {\n",
      "      key: \"predicted_species\"\n",
      "      value {\n",
      "        number_value: 0.0\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"species_probability_0\"\n",
      "      value {\n",
      "        number_value: 0.5506691017471723\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"species_probability_1\"\n",
      "      value {\n",
      "        number_value: 0.22587241877164385\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"species_probability_2\"\n",
      "      value {\n",
      "        number_value: 0.22345847948118383\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  records {\n",
      "    fields {\n",
      "      key: \"predicted_species\"\n",
      "      value {\n",
      "        number_value: 2.0\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"species_probability_0\"\n",
      "      value {\n",
      "        number_value: 0.22347717521202046\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"species_probability_1\"\n",
      "      value {\n",
      "        number_value: 0.22720021905738896\n",
      "      }\n",
      "    }\n",
      "    fields {\n",
      "      key: \"species_probability_2\"\n",
      "      value {\n",
      "        number_value: 0.5493226057305906\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Binary prediction response of the request using `split` with a filter:\n",
      "result {\n",
      "  columns: \"predicted_species\"\n",
      "  data {\n",
      "    values {\n",
      "      number_value: 0.0\n",
      "    }\n",
      "  }\n",
      "  data {\n",
      "    values {\n",
      "      number_value: 2.0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse the response message from the `recrods` request.\n",
    "response_message = ai_serving_pb2.PredictResponse()\n",
    "response_message.ParseFromString(prediction_response_records.content)\n",
    "print('Binary prediction response of the request using `records`:')\n",
    "print(response_message)\n",
    "\n",
    "print('-'*120)\n",
    "\n",
    "# Parse the response message from the `split` request.\n",
    "response_message = ai_serving_pb2.PredictResponse()\n",
    "response_message.ParseFromString(prediction_response_split.content)\n",
    "print('Binary prediction response of the request using `split` with a filter:')\n",
    "print(response_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"next-steps\"></a>Next steps\n",
    "\n",
    "I hope the tutoiral can help you to learn how to use the AI-Serving. If you have any questions, please open issues on this repository. Feedback and contributions to the project, no matter what kind, are always very welcome.\n",
    "\n",
    "**Star the [AI-Serving](https://github.com/autodeployai/ai-serving) project if it's helpful for you!!!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
